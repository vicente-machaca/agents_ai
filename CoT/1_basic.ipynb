{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the standard prompting, where models are asked to directly produce the final answer, 'Chain of Thought Prompting' encourages LLMs to generate intermediate reasoning steps before providing the final answer to a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"images/cot.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# en windows es necesario especificar la ruta del .env\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model is working well; however, previous models fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": question\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "message = completion.choices[0].message.content\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding more explanation (CoT). In the output we see the reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria started with 23 apples. They used 20 for lunch and bought 6 more.\n",
      "23 - 20 + 6 = 9. The cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "question_CoT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": question_CoT\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "message = completion.choices[0].message.content\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot CoT prompting is a technique that allows large language models (LLMs) to generate more accurate answers to questions. It does this by appending the words \"Let's think step by step.\" to the end of a question. This simple prompt helps the LLM to generate a chain of thought that answers the question, from which the LLM can then extract a more accurate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"images/zero-shot.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They started with 23 apples, used 20 for lunch, so they had 3 left.\n",
      "Then they bought 6 more apples. 3 + 6 = 9. The answer is 9 apples.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A: Let's think step by step.\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": question_CoT\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "message = completion.choices[0].message.content\n",
    "print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
