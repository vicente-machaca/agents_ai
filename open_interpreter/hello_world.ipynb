{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Open Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete tutorial: https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing#scrollTo=sasDzlwm0JAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpreter import interpreter\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Paste your OpenAI API key below.\n",
    "interpreter.llm.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mimics the behavior of OpenAI's code interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.auto_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example, it write a hello world program and runs it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteLLM requires an API key. Trying again with a dummy API key. In the future, if this fixes it, please set a dummy API key to prevent this message. (e.g `interpreter --api_key x` or `self.api_key = 'x'`)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 907, in completion\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 784, in completion\n    return self.streaming(\n           ^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 1025, in streaming\n    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 617, in _get_openai_client\n    _new_client = OpenAI(\n                  ^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/openai/_client.py\", line 105, in __init__\n    raise OpenAIError(\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/main.py\", line 1420, in completion\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/main.py\", line 1393, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 914, in completion\n    raise OpenAIError(\nlitellm.llms.OpenAI.openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/respond.py\", line 86, in respond\n    for chunk in interpreter.llm.run(messages_for_llm):\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 322, in run\n    yield from run_tool_calling_llm(self, params)\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/run_tool_calling_llm.py\", line 178, in run_tool_calling_llm\n    for chunk in llm.completions(**request_params):\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 466, in fixed_litellm_completions\n    raise first_error  # If all attempts fail, raise the first error\n    ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 443, in fixed_litellm_completions\n    yield from litellm.completion(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 1086, in wrapper\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 974, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/main.py\", line 2848, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 8199, in exception_type\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 6489, in exception_type\n    raise AuthenticationError(\nlitellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n\nThere might be an issue with your API key(s).\n\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here'. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:907\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 907\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:784\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming(\n\u001b[1;32m    785\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj,\n\u001b[1;32m    786\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    787\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    788\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    789\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m    790\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m    791\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    792\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    793\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    794\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:1025\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.streaming\u001b[0;34m(self, logging_obj, timeout, data, model, api_key, api_base, organization, client, max_retries, headers)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstreaming\u001b[39m(\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1014\u001b[0m     logging_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1024\u001b[0m ):\n\u001b[0;32m-> 1025\u001b[0m     openai_client: OpenAI \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_openai_client(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m         is_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1027\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1028\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   1029\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1030\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m   1031\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m   1032\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m   1033\u001b[0m     )\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:617\u001b[0m, in \u001b[0;36mOpenAIChatCompletion._get_openai_client\u001b[0;34m(self, is_async, api_key, api_base, timeout, max_retries, organization, client)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m     _new_client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m    618\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m    619\u001b[0m         base_url\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m    620\u001b[0m         http_client\u001b[38;5;241m=\u001b[39mlitellm\u001b[38;5;241m.\u001b[39mclient_session,\n\u001b[1;32m    621\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    622\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    623\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m## SAVE CACHE KEY\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/main.py:1420\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m   1415\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1416\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1417\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m   1418\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[1;32m   1419\u001b[0m     )\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/main.py:1393\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1393\u001b[0m         response \u001b[38;5;241m=\u001b[39m openai_chat_completions\u001b[38;5;241m.\u001b[39mcompletion(\n\u001b[1;32m   1394\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1395\u001b[0m             messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1396\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1397\u001b[0m             model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[1;32m   1398\u001b[0m             print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[1;32m   1399\u001b[0m             api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1400\u001b[0m             api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   1401\u001b[0m             acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[1;32m   1402\u001b[0m             logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[1;32m   1403\u001b[0m             optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[1;32m   1404\u001b[0m             litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[1;32m   1405\u001b[0m             logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[1;32m   1406\u001b[0m             timeout\u001b[38;5;241m=\u001b[39mtimeout,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m             custom_prompt_dict\u001b[38;5;241m=\u001b[39mcustom_prompt_dict,\n\u001b[1;32m   1408\u001b[0m             client\u001b[38;5;241m=\u001b[39mclient,  \u001b[38;5;66;03m# pass AsyncOpenAI, OpenAI client\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m             organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m   1410\u001b[0m             custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   1411\u001b[0m         )\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:914\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m error_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m--> 914\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    915\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39merror_text, headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[1;32m    916\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/respond.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(interpreter)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 ):\n\u001b[1;32m    111\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     raise Exception(\n\u001b[0m\u001b[1;32m    113\u001b[0m                         \u001b[0;34mf\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here'. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# yield from run_function_calling_llm(self, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrun_tool_calling_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/run_tool_calling_llm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(llm, request_params)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"choices\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfirst_error\u001b[0m  \u001b[0;31m# If all attempts fail, raise the first error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfirst_error\u001b[0m  \u001b[0;31m# If all attempts fail, raise the first error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m   1085\u001b[0m                         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m   1085\u001b[0m                         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/main.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2849\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   8197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8198\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8199\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   8197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8198\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8199\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease print hello world.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/core.py:191\u001b[0m, in \u001b[0;36mOpenInterpreter.chat\u001b[0;34m(self, message, display, stream, blocking)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_chat(message\u001b[38;5;241m=\u001b[39mmessage, display\u001b[38;5;241m=\u001b[39mdisplay)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# If stream=False, *pull* from the stream.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_chat(message\u001b[38;5;241m=\u001b[39mmessage, display\u001b[38;5;241m=\u001b[39mdisplay):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Return new messages\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/core.py:223\u001b[0m, in \u001b[0;36mOpenInterpreter._streaming_chat\u001b[0;34m(self, message, display)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_streaming_chat\u001b[39m(\u001b[38;5;28mself\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Sometimes a little more code -> a much better experience!\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# wraps the vanilla .chat(display=False) generator in a display.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Quite different from the plain generator stuff. So redirect to that\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display:\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m terminal_interface(\u001b[38;5;28mself\u001b[39m, message)\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# One-off message\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/terminal_interface/terminal_interface.py:157\u001b[0m, in \u001b[0;36mterminal_interface\u001b[0;34m(interpreter, message)\u001b[0m\n\u001b[1;32m    149\u001b[0m             message \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    152\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_path,\n\u001b[1;32m    154\u001b[0m             }\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mchat(message, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;66;03m# Is this for thine eyes?\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/core.py:259\u001b[0m, in \u001b[0;36mOpenInterpreter._streaming_chat\u001b[0;34m(self, message, display)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_messages_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# DISABLED because I think we should just not transmit images to non-multimodal models?\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# REENABLE this when multimodal becomes more common:\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# This is where it all happens!\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_respond_and_store()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Save conversation if we've turned conversation_history on\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# If it's the first message, set the conversation name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/core.py:318\u001b[0m, in \u001b[0;36mOpenInterpreter._respond_and_store\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m last_flag_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m respond(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# For async usage\u001b[39;00m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_event\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen Interpreter stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/interpreter/core/respond.py:112\u001b[0m, in \u001b[0;36mrespond\u001b[0;34m(interpreter)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    107\u001b[0m     interpreter\u001b[38;5;241m.\u001b[39moffline \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message\n\u001b[1;32m    110\u001b[0m ):\n\u001b[1;32m    111\u001b[0m     output \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThere might be an issue with your API key(s).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo reset your API key (we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        Mac/Linux: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport OPENAI_API_KEY=your-key-here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        Windows: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetx OPENAI_API_KEY your-key-here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m then restart terminal.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    116\u001b[0m     interpreter\u001b[38;5;241m.\u001b[39moffline \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot have access\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    117\u001b[0m ):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Check for invalid model in error message and then fallback.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 907, in completion\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 784, in completion\n    return self.streaming(\n           ^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 1025, in streaming\n    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 617, in _get_openai_client\n    _new_client = OpenAI(\n                  ^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/openai/_client.py\", line 105, in __init__\n    raise OpenAIError(\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/main.py\", line 1420, in completion\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/main.py\", line 1393, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py\", line 914, in completion\n    raise OpenAIError(\nlitellm.llms.OpenAI.openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/respond.py\", line 86, in respond\n    for chunk in interpreter.llm.run(messages_for_llm):\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 322, in run\n    yield from run_tool_calling_llm(self, params)\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/run_tool_calling_llm.py\", line 178, in run_tool_calling_llm\n    for chunk in llm.completions(**request_params):\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 466, in fixed_litellm_completions\n    raise first_error  # If all attempts fail, raise the first error\n    ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 443, in fixed_litellm_completions\n    yield from litellm.completion(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 1086, in wrapper\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 974, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/main.py\", line 2848, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 8199, in exception_type\n    raise e\n  File \"/opt/conda/lib/python3.11/site-packages/litellm/utils.py\", line 6489, in exception_type\n    raise AuthenticationError(\nlitellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n\nThere might be an issue with your API key(s).\n\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here'. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\n\n"
     ]
    }
   ],
   "source": [
    "interpreter.chat(\"Please print hello world.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
